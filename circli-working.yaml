version: 2.1

orbs:
  aws-s3: circleci/aws-s3@4.1.3
  aws-cli: circleci/aws-cli@4.0

workflows:
  dev-pipeline:
    jobs:
      - build-and-deploy-backend:
          context: dev-aws
          name: deploy-backend-dev
          stage: dev
          filters:
            branches:
              only: v2
      - build-and-deploy-frontend:
          context: dev-aws
          name: deploy-frontend-dev
          stage: dev
          filters:
            branches:
              only: v2
              
  prod-pipeline:
    jobs:
      - build-and-deploy-backend:
          context: prod-aws
          name: deploy-backend-prod
          stage: prod
          filters:
            branches:
              only: main
      - build-and-deploy-frontend:
          context: prod-aws
          name: deploy-frontend-prod
          stage: prod
          filters:
            branches:
              only: main

jobs:
  build-and-deploy-backend:
    docker:
      - image: cimg/python:3.11
    parameters:
      stage:
        type: string
    environment:
      AWS_DEFAULT_REGION: us-west-2
    steps:
      - checkout
      - restore_cache:
          keys:
            - pip-cache-{{ .Branch }}
            - pip-cache-
      - aws-cli/setup
      - run:
          name: Build and deploy
          no_output_timeout: 20m
          command: |
            set -e
            unset DBHELPER_SUFFIX UTILSHELPER_SUFFIX AUTHHELPER_SUFFIX COGNITOHELPER_SUFFIX
            DATE_SUFFIX=$(date +%y%m%d-%H%M)

            # Build layers
            cd backend/layers
            for layer in dbHelper utilsHelper authHelper cognitoHelper; do
              echo "Building layer: $layer"

              # Collect files to hash
              files_to_hash=""
              
              # Add .py files
              py_files=$(find "$layer" -maxdepth 1 -name '*.py' -print)
              for f in $py_files; do
                files_to_hash="$files_to_hash $f"
              done

              # Add requirements.txt if present
              if [ -f "$layer/requirements.txt" ]; then
                files_to_hash="$files_to_hash $layer/requirements.txt"
              fi

              # Calculate sha256 hash, fallback to timestamp if files are missing
              if [ -n "$files_to_hash" ]; then
                layer_hash=$(cat $files_to_hash 2>/dev/null | sha256sum | cut -d' ' -f1)
              else
                layer_hash=$(date +%s)
              fi

              # Set a variable for each layer's suffix (using uppercase for CloudFormation parameter conventions)
              uplayer=$(echo "$layer" | tr a-z A-Z | tr -d '_')
              eval "${uplayer}_SUFFIX=$layer_hash"
              export "${uplayer}_SUFFIX"

              ARTIFACT_PATH="s3://$ARTIFACTS_BUCKET/layers/${layer}-${layer_hash}.zip"
              if aws s3 ls "$ARTIFACT_PATH" >/dev/null; then
                echo "${layer}-${layer_hash}.zip already exists in S3, skipping build."                
                continue
              fi

              mkdir -p "$layer/python"
              
              # Install dependencies if requirements.txt exists
              if [ -f "$layer/requirements.txt" ]; then
                pip install -r "$layer/requirements.txt" -t "$layer/python/" --cache-dir ~/.cache/pip
              fi
              
              # Copy Python files
              cp "$layer"/*.py "$layer/python/" 2>/dev/null || true
              
              # Create zip
              (cd "$layer" && zip -rq "../${layer}.zip" python/)
              
              # Upload to S3
              aws s3 cp "${layer}.zip" "s3://$ARTIFACTS_BUCKET/layers/${layer}-${layer_hash}.zip"
              rm "${layer}.zip"
              
              # Clean up
              rm -rf "$layer/python"
            done
            
            # Build lambdas
            cd ../lambdas
            
            for dir in */; do
              echo "Processing $dir"
              
              # Install dependencies
              if [ -f "$dir/requirements.txt" ]; then
                pip install -r "$dir/requirements.txt" -t "$dir/" --cache-dir ~/.cache/pip
                find "$dir" -name "*.pyc" -delete
                find "$dir" -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
                rm -rf "$dir"/*.dist-info "$dir"/*.egg-info
              fi
              
              # Create and upload zip
              lambda_name=$(basename "$dir")
              (cd "$dir" && zip -rq "../${lambda_name}.zip" . -x "*.pyc" "__pycache__/*")
              aws s3 cp "${lambda_name}.zip" "s3://$ARTIFACTS_BUCKET/lambdas/${lambda_name}-${DATE_SUFFIX}.zip"
              rm "${lambda_name}.zip"
            done
            
            # Deploy with monitoring
            cd ../../
            
            # Upload nested templates to S3
            aws s3 cp cfn/templates/ s3://$ARTIFACTS_BUCKET/templates/ --recursive
            
            # Start deployment in background and monitor progress
            STACK_NAME="phours-<< parameters.stage >>"

            aws cloudformation deploy \
              --template-file cfn/template.yaml \
              --stack-name $STACK_NAME \
              --parameter-overrides ProjectName=phours EnvironmentName=<< parameters.stage >> ArtifactsBucket=$ARTIFACTS_BUCKET \
                DateSuffix=$DATE_SUFFIX \
                DbHelperSuffix=$DBHELPER_SUFFIX \
                UtilsHelperSuffix=$UTILSHELPER_SUFFIX \
                AuthHelperSuffix=$AUTHHELPER_SUFFIX \
                CognitoHelperSuffix=$COGNITOHELPER_SUFFIX \
              --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
              --no-fail-on-empty-changeset &
            
            DEPLOY_PID=$!
            
            # Monitor stack events while deployment runs
            echo "Monitoring CloudFormation deployment progress..."
            while kill -0 $DEPLOY_PID 2>/dev/null; do
              aws cloudformation describe-stack-events \
                --stack-name $STACK_NAME \
                --query 'StackEvents[0:3].[Timestamp,LogicalResourceId,ResourceStatus,ResourceStatusReason]' \
                --output table 2>/dev/null || true
              sleep 60
            done
            
            # Wait for deployment to complete and get exit code
            wait $DEPLOY_PID

  build-and-deploy-frontend:
    docker:
      - image: cimg/node:20.18
    parameters:
      stage:
        type: string
    environment:
      AWS_DEFAULT_REGION: us-west-2
    steps:
      - checkout
      - restore_cache:
          keys:
            - npm-cache-{{ .Branch }}-{{ checksum "v2/package-lock.json" }}
            - npm-cache-{{ .Branch }}-
      - aws-cli/setup
      - run:
          name: Build and deploy frontend
          command: |
            set -e
            cd v2
            npm ci
            npm run lint
            npm run build
            aws s3 sync dist/ s3://$FRONTEND_BUCKET_<< parameters.stage >> --delete
            aws cloudfront create-invalidation --distribution-id $CLOUDFRONT_DISTRIBUTION_<< parameters.stage >> --paths "/*"
      - save_cache:
          key: npm-cache-{{ .Branch }}-{{ checksum "v2/package-lock.json" }}
          paths:
            - ~/.npm

